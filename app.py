# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P0JzSyR8ECwJ5kAu1kVyd0li4ftV23gv
"""

!pip install streamlit

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# 
# st.title("Customer Profiling & Targeted Marketing Strategy")
# 
# st.write("This is a placeholder. Replace this with your actual app code.")
#

import pandas as pd

# Load the Customer_Profiling.csv file into a DataFrame
df = pd.read_csv('/content/Customer_Profiling.csv')

# Display the first 5 rows of the DataFrame
display(df.head())

print('Missing values before imputation:')
print(df.isnull().sum()[df.isnull().sum() > 0])

# Impute missing values with the median for skewed data
for col in df.columns:
    if df[col].isnull().any():
        if df[col].dtype in ['float64', 'int64']:
            median_val = df[col].median()
            df[col].fillna(median_val, inplace=True)

print('\nMissing values after imputation:')
print(df.isnull().sum()[df.isnull().sum() > 0])

display(df.head())

df.info()

# Identify numerical columns for outlier detection (excluding 'ID' and other non-value columns)
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
# Remove ID from numerical_cols if present, as it's an identifier not a measurement
if 'ID' in numerical_cols:
    numerical_cols.remove('ID')

print(f"Original DataFrame shape: {df.shape}")

# Calculate the 99.5th percentile for each numerical column
upper_bounds = df[numerical_cols].quantile(0.995)

# Filter out extreme outliers
df_filtered = df.copy()
for col in numerical_cols:
    df_filtered = df_filtered[df_filtered[col] <= upper_bounds[col]]

# Update the original DataFrame with the filtered data
df = df_filtered

print(f"DataFrame shape after removing extreme outliers: {df.shape}")

display(df.head())

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.histplot(df['Income'], kde=True)
plt.title('Distribution of Income (After Cleaning)')
plt.xlabel('Income')
plt.ylabel('Frequency / Density')
plt.show()

from datetime import datetime
import pandas as pd

# Convert 'Dt_Customer' to datetime objects
df['Dt_Customer_dt'] = pd.to_datetime(df['Dt_Customer'], format='%d-%m-%Y')

# Extract the year from 'Dt_Customer_dt'
enrollment_date_year = df['Dt_Customer_dt'].dt.year

# Get the current year
current_year = datetime.now().year

# Calculate 'Enrollment_Year'
df['Enrollment_Year'] = current_year - enrollment_date_year

print("New 'Enrollment_Year' column created.")
print(df[['Dt_Customer', 'Dt_Customer_dt', 'Enrollment_Year']].head())

# Drop the intermediate 'Dt_Customer_dt' column if it's no longer needed
df.drop(columns=['Dt_Customer_dt'], inplace=True)

from datetime import datetime
import pandas as pd

# Convert 'Dt_Customer' to datetime objects
df['Dt_Customer_dt'] = pd.to_datetime(df['Dt_Customer'], format='%d-%m-%Y')

# Get today's date
today = datetime.now()

# Calculate the difference in days
df['Customer_Tenure_Days'] = (today - df['Dt_Customer_dt']).dt.days

print("New 'Customer_Tenure_Days' column created.")
print(df[['Dt_Customer', 'Customer_Tenure_Days']].head())

# Drop the intermediate 'Dt_Customer_dt' column if not needed
df.drop(columns=['Dt_Customer_dt'], inplace=True, errors='ignore')

df.info()

df['Total_Expenditure'] = df['MntWines'] + \
                            df['MntFruits'] + \
                            df['MntMeatProducts'] + \
                            df['MntFishProducts'] + \
                            df['MntSweetProducts'] + \
                            df['MntGoldProds']

print("New 'Total_Expenditure' column created.")
print(df[['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds', 'Total_Expenditure']].head())

df['Customer_Tenure_Months'] = df['Customer_Tenure_Days'] / 30   # Approximate average days in a month

print("New 'Customer_Tenure_Months' column created.")
print(df[['Customer_Tenure_Days', 'Customer_Tenure_Months']].head())

df['Average_Monthly_Spend'] = df['Total_Expenditure'] / df['Customer_Tenure_Months']

print("New 'Average_Monthly_Spend' column created.")
print(df[['Total_Expenditure', 'Customer_Tenure_Months', 'Average_Monthly_Spend']].head())

df['Dependents'] = df['Kidhome'] + df['Teenhome']

print("New 'Dependents' column created.")
print(df[['Kidhome', 'Teenhome', 'Dependents']].head())

total_rows = df.shape[0]
df['Dependency_Ratio'] = df['Dependents'] / total_rows

print("New 'Dependency_Ratio' column created.")
print(df[['Dependents', 'Dependency_Ratio']].head())

df['Engagement_Score'] = (df['NumWebVisitsMonth'] * 0.4) + (df['NumStorePurchases'] * 0.6)

print("New 'Engagement_Score' column created.")
print(df[['NumWebVisitsMonth', 'NumStorePurchases', 'Engagement_Score']].head())

df['Campaign_Response'] = df[['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5']].max(axis=1)

print("New 'Campaign_Response' column created.")
print(df[['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Campaign_Response']].head())
print("\nValue counts for 'Campaign_Response':")
print(df['Campaign_Response'].value_counts())

print("Value counts before replacement:\n", df['Marital_Status'].value_counts())

df['Marital_Status'] = df['Marital_Status'].replace(['Absurd', 'Alone', 'YOLO'], 'Single')
df['Marital_Status'] = df['Marital_Status'].replace(['Married', 'Together'], 'Committed')

print("\nValue counts after replacement:\n", df['Marital_Status'].value_counts())

"""Added absurd, YOLO, Alone into Single and Married, together into Committed."""

# Perform one-hot encoding on 'Education' and 'Marital_Status'
df = pd.get_dummies(df, columns=['Education', 'Marital_Status'], drop_first=True, dtype=int)

print("DataFrame after one-hot encoding 'Education' and 'Marital_Status':")
print(df.head())

df['Has Kids'] = (df['Kidhome'] > 0).astype(int)
df['Has Teens'] = (df['Teenhome'] > 0).astype(int)

print("New 'Has Kids' column created:")
print(df[['Kidhome', 'Has Kids']].head())
print("\nNew 'Has Teens' column created:")
print(df[['Teenhome', 'Has Teens']].head())

from sklearn.preprocessing import MinMaxScaler

# Columns to normalize
columns_to_normalize = ['Income', 'Total_Expenditure', 'Customer_Tenure_Months']

# Initialize the MinMaxScaler
scaler = MinMaxScaler()

# Apply Min-Max scaling to the selected columns
df[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])

print("Columns normalized successfully:")
print(df[columns_to_normalize].head())

df.info()

from datetime import datetime

current_year = datetime.now().year
df['Age'] = current_year - df['Year_Birth']

print("New 'Age' column created.")
print(df[['Year_Birth', 'Age']].head())

columns_to_drop = [
    'ID',
    'Year_Birth',
    'Kidhome',
    'Teenhome',
    'Dt_Customer',
    'AcceptedCmp1',
    'AcceptedCmp2',
    'AcceptedCmp3',
    'AcceptedCmp4',
    'AcceptedCmp5',
    'Z_CostContact',
    'Z_Revenue',
    'Customer_Tenure_Days'
]

df2 = df.copy() # Create a copy of the original data
df2.drop(columns=columns_to_drop, inplace=True)

print(f"Columns dropped successfully from df2. New df2 DataFrame shape: {df2.shape}")
print("Remaining columns in df2:", df2.columns.tolist())

from sklearn.decomposition import PCA
import pandas as pd

# Initialize PCA with 2 components
pca = PCA(n_components=2)

# Fit and transform df2
df2_pca = pca.fit_transform(df2)

# Create a DataFrame for the PCA results
df2_pca_df = pd.DataFrame(data=df2_pca, columns=['Principal_Component_1', 'Principal_Component_2'])

print(f"Shape of df2 after PCA: {df2_pca_df.shape}")
print("First 5 rows of PCA-transformed df2:")
display(df2_pca_df.head())

print("Explained Variance Ratio by Principal Component:")
print(pca.explained_variance_ratio_)
print(f"Total Explained Variance: {pca.explained_variance_ratio_.sum():.4f}")

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Create a DataFrame for plotting the PCA results
pca_response_df = pd.DataFrame(data=df2_pca, columns=['Principal_Component_1', 'Principal_Component_2'])

# Add the 'Response' column from df3 to the PCA DataFrame
pca_response_df['Response'] = df3['Response'].values

# Plot the principal components colored by the 'Response' variable
plt.figure(figsize=(10, 8))
sns.scatterplot(
    x='Principal_Component_1',
    y='Principal_Component_2',
    hue='Response',
    palette='coolwarm',
    data=pca_response_df,
    s=100,
    alpha=0.7
)
plt.title('PCA Components Colored by Campaign Response')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.grid(True)
plt.legend(title='Response (0=No, 1=Yes)')
plt.show()

import statsmodels.api as sm

# Define the target variable (dependent variable)
Y = df2['Response']

# Define independent variables (features) by dropping redundant/collinear columns
X = df2.drop(columns=[
    'Response',
    'Campaign_Response',
    'MntWines',
    'MntFruits',
    'MntMeatProducts',
    'MntFishProducts',
    'MntSweetProducts',
    'MntGoldProds',
    'NumWebVisitsMonth',
    'NumStorePurchases',
    'Dependency_Ratio',
    'Total_Expenditure'
])

# Add a constant to the independent variables for the intercept
X = sm.add_constant(X)

# Initialize and fit the logistic regression model
logit_model = sm.Logit(Y, X)
result = logit_model.fit()

# Print the model summary
print(result.summary())

# Create a new DataFrame 'df3' as a copy of 'df2'
df3 = df2.copy()

# Define columns to drop from df3. These are the constituent parts or highly correlated features
# that are now represented by engineered features, or features redundant with the target.
columns_to_drop_from_df3 = [
    'MntWines',
    'MntFruits',
    'MntMeatProducts',
    'MntFishProducts',
    'MntSweetProducts',
    'MntGoldProds',
    'NumWebVisitsMonth',
    'NumStorePurchases',
    'Dependency_Ratio', # Redundant with Dependents (scaled version)
    'Campaign_Response' # Highly correlated with Response, so drop for independent variable X
]

# Drop redundant features from df3
df3.drop(columns=columns_to_drop_from_df3, inplace=True, errors='ignore')

print(f"DataFrame 'df3' created with shape: {df3.shape}")
print(f"Remaining columns in df3: {df3.columns.tolist()}\n")

# Redefine 'X' and 'Y' for logistic regression using df3
Y = df3['Response']
# X includes all columns in df3 except 'Response'
X = df3.drop(columns=['Response'])

# Add a constant to the independent variables for the intercept
X = sm.add_constant(X)

# Initialize and fit the logistic regression model again
logit_model_refined = sm.Logit(Y, X)
result_refined = logit_model_refined.fit()

# Print the refined model summary
print(result_refined.summary())

from sklearn.metrics import roc_auc_score, precision_score, recall_score, roc_curve
import matplotlib.pyplot as plt

# Get predicted probabilities
Y_pred_proba = result_refined.predict(X)

# Convert probabilities to binary predictions using a threshold (e.g., 0.5)
Y_pred = (Y_pred_proba >= 0.5).astype(int)

# Calculate AUC
auc = roc_auc_score(Y, Y_pred_proba)

# Calculate Precision
precision = precision_score(Y, Y_pred)

# Calculate Recall
recall = recall_score(Y, Y_pred)

print(f"AUC: {auc:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")

# Plot ROC curve
fpr, tpr, thresholds = roc_curve(Y, Y_pred_proba)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='orange', label=f'ROC Curve (AUC = {auc:.2f})')
plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--', label='Random Classifier')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend()
plt.grid(True)
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

# Initialize the Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
rf_classifier.fit(X_train, Y_train)

# Make predictions on the test set
Y_pred_rf = rf_classifier.predict(X_test)

# Evaluate the model
accuracy_rf = accuracy_score(Y_test, Y_pred_rf)
report_rf = classification_report(Y_test, Y_pred_rf)

print(f"Random Forest Classifier Accuracy: {accuracy_rf:.4f}")
print("\nRandom Forest Classifier Classification Report:\n", report_rf)

df3.info()

df2.info()

df3['Response'].value_counts(normalize=True)

from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import MinMaxScaler
import pandas as pd

# Identify numerical columns in X_train that are not yet scaled (excluding binary/one-hot encoded and the constant term)
numerical_cols_to_scale_now = [
    'Recency',
    'NumDealsPurchases',
    'NumWebPurchases',
    'NumCatalogPurchases',
    'Enrollment_Year',
    'Average_Monthly_Spend',
    'Dependents',
    'Engagement_Score',
    'Age'
]

# Create a copy of X_train to avoid modifying the original and to store scaled data
X_train_scaled = X_train.copy()

# Initialize a new MinMaxScaler for the remaining columns
scaler_new = MinMaxScaler()

# Apply Min-Max scaling to the selected unscaled numerical columns in X_train_scaled
X_train_scaled[numerical_cols_to_scale_now] = scaler_new.fit_transform(X_train_scaled[numerical_cols_to_scale_now])

# Initialize SMOTE
sm = SMOTE(random_state=42)

# Apply SMOTE to the scaled training data
X_train_res, y_train_res = sm.fit_resample(X_train_scaled, Y_train)

print(f"Original X_train shape: {X_train.shape}")
print(f"Resampled X_train_res shape: {X_train_res.shape}")
print(f"Resampled y_train_res value counts:\n{y_train_res.value_counts()}")

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Initialize the Logistic Regression model
log_reg = LogisticRegression(max_iter=1000, random_state=42) # Added random_state for reproducibility

# Fit the model on the balanced training data
log_reg.fit(X_train_res, y_train_res)

# Make predictions on the original (unresampled) test set
Y_pred_log_reg = log_reg.predict(X_test)

# Evaluate the model
accuracy_log_reg = accuracy_score(Y_test, Y_pred_log_reg)
report_log_reg = classification_report(Y_test, Y_pred_log_reg)

print(f"Logistic Regression (Balanced Data) Accuracy: {accuracy_log_reg:.4f}")
print("\nLogistic Regression (Balanced Data) Classification Report:\n", report_log_reg)



"""# Task
The user has approved the previous step. The next step is to calculate prediction probabilities using the trained Random Forest model on the `X` dataset (which was derived from `df3`), and then add these probabilities as a new column named `Prediction_Probability` to `df3`.

This will be followed by displaying `df3.head()` and the descriptive statistics of the new column.

## df3_creation_and_prediction_probability

### Subtask:
Re-create df3 from df2 (if necessary), define X and Y, calculate prediction probabilities using the trained Random Forest model on the full X dataset, and add 'Prediction_Probability' to df3. Then display the head of df3 and descriptive statistics of the new column.

## Summary:

### Data Analysis Key Findings
- The `Prediction_Probability` column was successfully added to `df3`, incorporating prediction probabilities generated by the Random Forest model for the `X` dataset.
- The initial rows of `df3` were displayed, confirming the successful integration of the new column.
- Descriptive statistics for the `Prediction_Probability` column were computed and displayed, providing an overview of its distribution within the dataset.

### Insights or Next Steps
- The `Prediction_Probability` column can now be leveraged to assess the model's confidence in its predictions, aiding in the identification of potentially uncertain classifications.
- Future analysis could involve visualizing the distribution of these probabilities or investigating records with particularly high or low probabilities to gain deeper insights into the model's performance and behavior.
"""

df3.info()

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
import statsmodels.api as sm # Required for sm.add_constant

# Re-define Y and X from df3 (as used for modeling)
Y = df3['Response']
X = df3.drop(columns=['Response'])
X = sm.add_constant(X) # Add constant as done previously for model training

# Split the data into training and testing sets (necessary to train rf_classifier)
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

# Re-initialize and train the Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train, Y_train)

# Get prediction probabilities for the positive class (class 1) for the full dataset X
df3['Prediction_Probability'] = rf_classifier.predict_proba(X)[:, 1]

print("New 'Prediction_Probability' column added to df3.")
display(df3.head())
print("\nDescriptive statistics for 'Prediction_Probability':")
display(df3['Prediction_Probability'].describe())

cluster_features = [
    'Income',
    'Recency',
    'NumDealsPurchases',
    'NumWebPurchases',
    'NumCatalogPurchases',
    'Total_Expenditure',
    'Customer_Tenure_Months',
    'Average_Monthly_Spend',
    'Dependents',
    'Engagement_Score',
    'Age',
    'Has Kids',
    'Has Teens',
    'Prediction_Probability' # Using the generated Prediction_Probability
]

cluster_df = df3[cluster_features].copy()

print(f"DataFrame 'cluster_df' created with shape: {cluster_df.shape}")
print("First 5 rows of cluster_df:")
display(cluster_df.head())

from sklearn.preprocessing import StandardScaler

cluster_scaler = StandardScaler()
cluster_scaled = cluster_scaler.fit_transform(cluster_df)

print("Segmentation features scaled successfully. Shape of cluster_scaled:", cluster_scaled.shape)
print("First 5 rows of scaled data:")
print(cluster_scaled[:5])

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns

# Determine the optimal number of clusters using the Elbow Method
wcss = []
# Try a range of k values, e.g., from 1 to 10
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42, n_init=10)
    kmeans.fit(cluster_scaled)
    wcss.append(kmeans.inertia_)

# Plot the Elbow Method
plt.figure(figsize=(10, 6))
plt.plot(range(1, 11), wcss, marker='o', linestyle='--')
plt.title('Elbow Method for Optimal K')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('WCSS (Within-Cluster Sum of Squares)')
plt.grid(True)
plt.show()

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import seaborn as sns

# Determine the optimal number of clusters using the Silhouette Method
silhouette_scores = []
# Silhouette score is not defined for k=1, so we start from 2
for i in range(2, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42, n_init=10)
    kmeans.fit(cluster_scaled)
    score = silhouette_score(cluster_scaled, kmeans.labels_)
    silhouette_scores.append(score)

# Plot the Silhouette Scores
plt.figure(figsize=(10, 6))
plt.plot(range(2, 11), silhouette_scores, marker='o', linestyle='--')
plt.title('Silhouette Method for Optimal K')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Silhouette Score')
plt.grid(True)
plt.show()

# Print the optimal K based on the highest silhouette score
optimal_k_silhouette = range(2, 11)[silhouette_scores.index(max(silhouette_scores))]
print(f"Optimal number of clusters (K) based on Silhouette Score: {optimal_k_silhouette}")

"""I'm taking K=3"""

from sklearn.cluster import KMeans

k_optimal = 3 # Using k=3 as decided from the Elbow method
kmeans = KMeans(n_clusters=k_optimal, init='k-means++', random_state=42, n_init=10)
cluster_df['Cluster'] = kmeans.fit_predict(cluster_scaled)

print(f"K-Means clustering applied with {k_optimal} clusters.")
print("First 5 rows of df with new 'Cluster' column:")
display(cluster_df.head())
print("\nValue counts for 'Cluster' column:")
display(cluster_df['Cluster'].value_counts())

display(cluster_df.head(15))

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

# Apply PCA to reduce to 2 dimensions for visualization
pca = PCA(n_components=2)
cluster_pca = pca.fit_transform(cluster_scaled)

# Create a DataFrame for plotting the PCA results
pca_df = pd.DataFrame(data=cluster_pca, columns=['Principal_Component_1', 'Principal_Component_2'])

# Add the cluster labels to the PCA DataFrame
pca_df['Cluster'] = kmeans.labels_

# Plot the clusters
plt.figure(figsize=(10, 8))
sns.scatterplot(
    x='Principal_Component_1',
    y='Principal_Component_2',
    hue='Cluster',
    palette='viridis',
    data=pca_df,
    s=100,
    alpha=0.7
)
plt.title('Customer Clusters (PCA Reduced Dimensions)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.grid(True)
plt.legend(title='Cluster')
plt.show()

# Reiterate the textual interpretation of clusters
print("\n--- Detailed Interpretation of Clusters ---\n")
cluster_analysis = cluster_df.groupby('Cluster').mean()
print("Cluster Profiles (Mean values of features per cluster):")
display(cluster_analysis)

print("\n### Cluster 0: \"Budget-Conscious Families with Young Children\"")
print("*   **Size:** Largest segment (1009 customers).")
print("*   **Income & Expenditure:** Lowest average income, total expenditure, and average monthly spend.")
print("*   **Dependents:** Highest number of dependents, with a high proportion having kids (75%) and a significant portion having teens (42%). They appear to be younger families.")
print("*   **Purchases:** Lowest in web and catalog purchases, but moderate in deals purchases, indicating a preference for cost-saving options.")
print("*   **Engagement:** Lowest overall engagement score.")
print("*   **Age:** Youngest average age.")
print("*   **Prediction Probability:** Lowest predicted campaign response probability (0.09).")
print("    *   **Profile:** These are likely younger families with lower disposable income, focused on deals and less active in online or catalog shopping. They are the least likely to respond to campaigns.\n")

print("### Cluster 1: \"High-Value, Campaign-Responsive Individuals/Couples\"")
print("*   **Size:** Smallest segment (492 customers).")
print("*   **Income & Expenditure:** Highest average income, highest total expenditure, and highest average monthly spend.")
print("*   **Dependents:** Very few dependents, with very low 'Has Kids' and 'Has Teens' proportions, suggesting singles or couples without children.")
print("*   **Purchases:** Lowest in deals purchases, but high in web and very high in catalog purchases, indicating a preference for convenience and perhaps higher-end products.")
print("*   **Engagement:** Moderate engagement score.")
print("*   **Age:** Middle average age.")
print("*   **Prediction Probability:** Highest predicted campaign response probability (0.286).")
print("    *   **Profile:** This is the most affluent segment, spending heavily across channels, particularly catalog. They are likely individuals or couples without significant family obligations and are the most receptive to campaigns. This is a prime target segment.\n")

print("### Cluster 2: \"Engaged Families with Teenagers\"")
print("*   **Size:** Middle segment (639 customers).")
print("*   **Income & Expenditure:** Middle average income, total expenditure, and average monthly spend.")
print("*   **Dependents:** High number of dependents, specifically with the highest proportion having teens (92%) and lowest having young kids (24%). These are likely older families with older children.")
print("*   **Purchases:** Highest in deals purchases, highest in web purchases, and moderate in catalog purchases.")
print("*   **Engagement:** Highest overall engagement score.")
print("*   **Age:** Oldest average age.")
print("*   **Prediction Probability:** Middle predicted campaign response probability (0.12).")
print("    *   **Profile:** This segment consists of older families with teenagers, showing high engagement across purchasing channels and a strong interest in deals. They represent a significant segment for engagement strategies.\n")

print("\n--- Summary of Segmentation ---")
print("This segmentation provides actionable insights for targeted marketing efforts:")
print("*   **Cluster 1** represents your most valuable and responsive customers. Strategies should focus on retention and personalized offers that align with their high spending habits and receptiveness.")
print("*   **Cluster 2** is a highly engaged segment, particularly effective with web purchases and deals. Campaigns can focus on family-oriented products or services, leveraging their engagement.")
print("*   **Cluster 0** is the largest but least responsive and lowest-spending segment. Strategies might need to focus on value propositions, loyalty programs, or different product offerings to convert them, acknowledging their budget-conscious nature and family needs.")



import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import statsmodels.api as sm

# Ensure X and Y are defined from df3
Y = df3['Response']
X = df3.drop(columns=['Response'])
X = sm.add_constant(X) # Add constant if not already present, consistent with earlier models

# Split data to ensure rf_classifier can be re-trained consistently
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

# Re-initialize and train the Random Forest Classifier (if not already in memory)
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train, Y_train)

# Get feature importances
feature_importances = rf_classifier.feature_importances_

# Create a DataFrame for better visualization
features_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': feature_importances
})

# Sort features by importance
features_df = features_df.sort_values(by='Importance', ascending=False)

# Plotting the feature importances
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=features_df)
plt.title('Feature Importance for Predicting Campaign Response (Random Forest)')
plt.xlabel('Relative Importance')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()



"""# Task
Redefine X and Y from df3, excluding 'Prediction_Probability' from X. Split the data into training and testing sets. Train a Random Forest Classifier on the new training data. Calculate and plot the feature importances of the trained model.

## feature_importance_without_prediction_probability

### Subtask:
Redefine X and Y from df3, excluding 'Prediction_Probability' from X. Split data, train Random Forest Classifier, calculate feature importances, and plot them.

## Summary:

### Data Analysis Key Findings
*   Feature importances were calculated using a Random Forest Classifier trained on `df3`, excluding 'Prediction_Probability' from the features.
*   This analysis helps in identifying which input features are most influential in the model's predictions. The exact ranking and importance scores of features would highlight their relative contribution.

### Insights or Next Steps
*   Focus on the most important features identified for further in-depth analysis, such as exploring their distributions or interactions.
*   Consider using the highly important features for feature selection to build a more parsimonious model, potentially improving interpretability and reducing overfitting.
"""

print("Variables dropped when applying logistic regression (from df3):")
dropped_vars = [
    'MntWines',
    'MntFruits',
    'MntMeatProducts',
    'MntFishProducts',
    'MntSweetProducts',
    'MntGoldProds',
    'NumWebVisitsMonth',
    'NumStorePurchases',
    'Dependency_Ratio',
    'Campaign_Response'
]
for var in dropped_vars:
    print(f"- {var}")



"""# Task
Create an interactive widget where users can input customer details and receive a prediction probability from the Random Forest model.

## create_prediction_widget

### Subtask:
Create an interactive widget where users can input customer details and receive a prediction probability from the Random Forest model.

## Summary:

### Data Analysis Key Findings
*   A new subtask has been initiated with the objective of creating an interactive prediction widget.
*   This widget's primary function will be to accept customer details as input and generate prediction probabilities using a pre-existing Random Forest model.

### Insights or Next Steps
*   The immediate next step involves the development and implementation of this interactive widget, integrating the trained Random Forest model to provide an accessible interface for predictions.

# Task
Create an interactive widget that allows users to input customer details for the features used by the Random Forest model and then displays the predicted campaign response probability.

## define_widget_inputs

### Subtask:
Identify and define the input fields for the widget based on the significant features used by the Random Forest model.

**Reasoning**:
Extract the column names from the `X` DataFrame to identify the input fields for the widget, as specified in the instructions.
"""

feature_columns = X.columns.tolist()

print("Identified feature columns for the widget input:")
print(feature_columns)

"""## create_preprocessing_function

### Subtask:
Develop a function to preprocess raw user inputs, including one-hot encoding categorical features and scaling numerical features using the previously fitted scalers and encoders.

**Reasoning**:
To develop the `preprocess_inputs` function, I need to know the exact feature names and their order that the `rf_classifier` model expects. Printing `rf_classifier.feature_names_in_` will provide this information, which is crucial for correctly structuring the preprocessed input data.
"""

print("Feature names and their order required by the Random Forest model:")
print(rf_classifier.feature_names_in_)

"""**Reasoning**:
Now that the `rf_classifier.feature_names_in_` have been identified, the next step is to create the `preprocess_inputs` function. This function will take raw user inputs, apply the necessary scaling and one-hot encoding, and then arrange the features in the exact order required by the `rf_classifier` model.


"""

import pandas as pd

def preprocess_inputs(
    Income, Recency, NumDealsPurchases, NumWebPurchases, NumCatalogPurchases,
    Complain, Enrollment_Year, Total_Expenditure, Customer_Tenure_Months,
    Average_Monthly_Spend, Dependents, Engagement_Score, Education,
    Marital_Status, Has_Kids, Has_Teens, Age, Prediction_Probability
):
    # Create a dictionary from the raw inputs
    data = {
        'Income': Income,
        'Recency': Recency,
        'NumDealsPurchases': NumDealsPurchases,
        'NumWebPurchases': NumWebPurchases,
        'NumCatalogPurchases': NumCatalogPurchases,
        'Complain': Complain,
        'Enrollment_Year': Enrollment_Year,
        'Total_Expenditure': Total_Expenditure,
        'Customer_Tenure_Months': Customer_Tenure_Months,
        'Average_Monthly_Spend': Average_Monthly_Spend,
        'Dependents': Dependents,
        'Engagement_Score': Engagement_Score,
        'Has Kids': Has_Kids,
        'Has Teens': Has_Teens,
        'Age': Age,
        'Prediction_Probability': Prediction_Probability
    }

    # Convert to pandas Series for easier manipulation and to handle single row input
    input_series = pd.Series(data)

    # Add 'const' column
    input_series['const'] = 1.0

    # Apply MinMaxScaler to previously scaled columns
    # The scaler was fitted on 'Income', 'Total_Expenditure', 'Customer_Tenure_Months'
    # Ensure the input to transform is 2D (e.g., a DataFrame with one row)
    cols_to_scale = ['Income', 'Total_Expenditure', 'Customer_Tenure_Months']
    scaled_values = scaler.transform(pd.DataFrame(input_series[cols_to_scale]).T)
    input_series[cols_to_scale] = scaled_values[0]

    # Recreate one-hot encoded columns for 'Education' (drop_first=True, base is '2n Cycle')
    input_series['Education_Basic'] = 0
    input_series['Education_Graduation'] = 0
    input_series['Education_Master'] = 0
    input_series['Education_PhD'] = 0
    if Education == 'Basic':
        input_series['Education_Basic'] = 1
    elif Education == 'Graduation':
        input_series['Education_Graduation'] = 1
    elif Education == 'Master':
        input_series['Education_Master'] = 1
    elif Education == 'PhD':
        input_series['Education_PhD'] = 1
    # '2n Cycle' (default/dropped category) will have all 0s

    # Recreate one-hot encoded columns for 'Marital_Status' (drop_first=True, base is 'Committed')
    input_series['Marital_Status_Divorced'] = 0
    input_series['Marital_Status_Single'] = 0
    input_series['Marital_Status_Widow'] = 0

    if Marital_Status == 'Divorced':
        input_series['Marital_Status_Divorced'] = 1
    elif Marital_Status == 'Single':
        input_series['Marital_Status_Single'] = 1
    elif Marital_Status == 'Widow':
        input_series['Marital_Status_Widow'] = 1
    # 'Committed' (default/dropped category) will have all 0s

    # Define the exact order of features as expected by the model
    # Note: rf_classifier.feature_names_in_ includes 'const' and 'Prediction_Probability'
    # We need to make sure 'const' is at the beginning and other engineered features are included.
    model_feature_order = list(rf_classifier.feature_names_in_)

    # Create a DataFrame with a single row, ensuring all features are present and in the correct order
    # Use .loc to avoid SettingWithCopyWarning if input_series has been viewed as a slice
    preprocessed_df = pd.DataFrame([input_series[model_feature_order].values], columns=model_feature_order)

    return preprocessed_df

print("The 'preprocess_inputs' function has been defined.")

"""## build_interactive_widget

### Subtask:
Construct the interactive widget using `ipywidgets` to allow users to enter values for each input feature.

**Reasoning**:
To create the interactive widgets with appropriate default, min, and max values, I need to inspect the descriptive statistics of the numerical features in `df3`.
"""

print("Descriptive statistics for df3:")
display(df3.describe())

# Define categorical options
education_options = ['2n Cycle', 'Basic', 'Graduation', 'Master', 'PhD']
marital_status_options = ['Committed', 'Divorced', 'Single', 'Widow']

print("\nCategorical options and binary initial states defined.")

"""**Reasoning**:
Now that the descriptive statistics are available, I will create interactive widgets for each input feature, setting appropriate ranges and default values based on the data, and arrange them using `ipywidgets`.


"""

import ipywidgets as widgets
from IPython.display import display, clear_output
import pandas as pd

# --- Create Widgets for each feature ---

# Numerical Features (Sliders)
w_income = widgets.FloatSlider(
    value=df3['Income'].mean(),
    min=df3['Income'].min(),
    max=df3['Income'].max(),
    step=0.01,
    description='Income (Scaled):',
    continuous_update=False,
    orientation='horizontal',
    readout=True,
    readout_format='.2f',
)
w_recency = widgets.IntSlider(
    value=int(df3['Recency'].mean()),
    min=int(df3['Recency'].min()),
    max=int(df3['Recency'].max()),
    step=1,
    description='Recency (Days):',
    continuous_update=False,
    orientation='horizontal',
    readout=True,
)
w_num_deals_purchases = widgets.IntSlider(
    value=int(df3['NumDealsPurchases'].mean()),
    min=int(df3['NumDealsPurchases'].min()),
    max=int(df3['NumDealsPurchases'].max()),
    step=1,
    description='Deals Purchases:',
    continuous_update=False,
    orientation='horizontal',
    readout=True,
)
w_num_web_purchases = widgets.IntSlider(
    value=int(df3['NumWebPurchases'].mean()),
    min=int(df3['NumWebPurchases'].min()),
    max=int(df3['NumWebPurchases'].max()),
    step=1,
    description='Web Purchases:',
    continuous_update=False,
    orientation='horizontal',
    readout=True,
)
w_num_catalog_purchases = widgets.IntSlider(
    value=int(df3['NumCatalogPurchases'].mean()),
    min=int(df3['NumCatalogPurchases'].min()),
    max=int(df3['NumCatalogPurchases'].max()),
    step=1,
    description='Catalog Purchases:',
    continuous_update=False,
    orientation='horizontal',
    readout=True,
)
w_enrollment_year = widgets.IntSlider(
    value=int(df3['Enrollment_Year'].mean()),
    min=int(df3['Enrollment_Year'].min()),
    max=int(df3['Enrollment_Year'].max()),
    step=1,
    description='Enrollment Year:',
    continuous_update=False,
    orientation='horizontal',
    readout=True,
)
w_total_expenditure = widgets.FloatSlider(
    value=df3['Total_Expenditure'].mean(),
    min=df3['Total_Expenditure'].min(),
    max=df3['Total_Expenditure'].max(),
    step=0.01,
    description='Total Expenditure (Scaled):',
    continuous_update=False,
    orientation='horizontal',
    readout=True,
    readout_format='.2f',
)
w_customer_tenure_months = widgets.FloatSlider(
    value=df3['Customer_Tenure_Months'].mean(),
    min=df3['Customer_Tenure_Months'].min(),
    max=df3['Customer_Tenure_Months'].max(),
    step=0.01,
    description='Tenure (Months, Scaled):',
    continuous_update=False,
    orientation='horizontal',
    readout=True,
    readout_format='.2f',
)
w_average_monthly_spend = widgets.FloatSlider(
    value=df3['Average_Monthly_Spend'].mean(),
    min=df3['Average_Monthly_Spend'].min(),
    max=df3['Average_Monthly_Spend'].max(),
    step=0.1,
    description='Avg Monthly Spend:',
    continuous_update=False,
    orientation='horizontal',
    readout=True,
    readout_format='.1f',
)
w_dependents = widgets.IntSlider(
    value=int(df3['Dependents'].mean()),
    min=int(df3['Dependents'].min()),
    max=int(df3['Dependents'].max()),
    step=1,
    description='Dependents:',
    continuous_update=False,
    orientation='horizontal',
    readout=True,
)
w_engagement_score = widgets.FloatSlider(
    value=df3['Engagement_Score'].mean(),
    min=df3['Engagement_Score'].min(),
    max=df3['Engagement_Score'].max(),
    step=0.1,
    description='Engagement Score:',
    continuous_update=False,
    orientation='horizontal',
    readout=True,
    readout_format='.1f',
)
w_age = widgets.IntSlider(
    value=int(df3['Age'].mean()),
    min=int(df3['Age'].min()),
    max=int(df3['Age'].max()),
    step=1,
    description='Age:',
    continuous_update=False,
    orientation='horizontal',
    readout=True,
)

# Categorical Features (Dropdowns)
w_education = widgets.Dropdown(
    options=education_options,
    value='Graduation', # Most common category in the dataset
    description='Education:',
)
w_marital_status = widgets.Dropdown(
    options=marital_status_options,
    value='Committed', # Most common category after consolidation
    description='Marital Status:',
)

# Binary Features (Checkboxes)
w_complain = widgets.Checkbox(
    value=False, # Majority is False
    description='Has Complained:',
    disabled=False,
)
w_has_kids = widgets.Checkbox(
    value=False, # Roughly 50/50 but 0 is slightly more common
    description='Has Kids:',
    disabled=False,
)
w_has_teens = widgets.Checkbox(
    value=False, # Roughly 50/50 but 0 is slightly more common
    description='Has Teens:',
    disabled=False,
)

# Prediction Button
predict_button = widgets.Button(
    description='Predict Response',
    button_style='success', # 'success', 'info', 'warning', 'danger' or ''
    tooltip='Click to predict campaign response',
)

# Output Widget
output_widget = widgets.Output()

print("Interactive widgets created for customer details input.")

"""**Reasoning**:
With all individual widgets created, the next step is to define the prediction function and then arrange these widgets into a coherent interactive interface using `ipywidgets` layout containers.


"""

import ipywidgets as widgets
from IPython.display import display, clear_output

def on_predict_button_clicked(b):
    with output_widget:
        clear_output()
        # Get values from widgets
        # Note: Prediction_Probability is set to a default value (e.g., 0) as it's an output, not an input for the raw prediction
        user_input_df = preprocess_inputs(
            Income=w_income.value,
            Recency=w_recency.value,
            NumDealsPurchases=w_num_deals_purchases.value,
            NumWebPurchases=w_num_web_purchases.value,
            NumCatalogPurchases=w_num_catalog_purchases.value,
            Complain=int(w_complain.value),
            Enrollment_Year=w_enrollment_year.value,
            Total_Expenditure=w_total_expenditure.value,
            Customer_Tenure_Months=w_customer_tenure_months.value,
            Average_Monthly_Spend=w_average_monthly_spend.value,
            Dependents=w_dependents.value,
            Engagement_Score=w_engagement_score.value,
            Education=w_education.value,
            Marital_Status=w_marital_status.value,
            Has_Kids=int(w_has_kids.value),
            Has_Teens=int(w_has_teens.value),
            Age=w_age.value,
            Prediction_Probability=0.0 # Placeholder, as this is what we're predicting or not directly inputting
        )

        # Make prediction
        prediction_proba = rf_classifier.predict_proba(user_input_df)[:, 1]
        prediction_label = rf_classifier.predict(user_input_df)[0]

        print(f"Predicted Probability of Campaign Response: {prediction_proba[0]:.4f}")
        print(f"Predicted Campaign Response (0=No, 1=Yes): {prediction_label}")

# Attach the prediction function to the button click event
predict_button.on_click(on_predict_button_clicked)

# Arrange widgets
input_widgets_left = widgets.VBox([
    w_income, w_recency, w_num_deals_purchases, w_num_web_purchases, w_num_catalog_purchases,
    w_enrollment_year, w_total_expenditure, w_customer_tenure_months
])
input_widgets_right = widgets.VBox([
    w_average_monthly_spend, w_dependents, w_engagement_score, w_age,
    w_education, w_marital_status, w_complain, w_has_kids, w_has_teens
])

input_layout = widgets.HBox([input_widgets_left, input_widgets_right])

# Display the widget
print("Interactive Prediction Widget:")
display(input_layout, predict_button, output_widget)

"""## Summary:

### Data Analysis Key Findings
*   The Random Forest model utilizes 24 significant features for prediction, including `Income`, `Recency`, `NumDealsPurchases`, `NumWebPurchases`, `NumCatalogPurchases`, `Complain`, `Enrollment_Year`, `Total_Expenditure`, `Customer_Tenure_Months`, `Average_Monthly_Spend`, `Dependents`, `Engagement_Score`, various education and marital status categories, `Has Kids`, `Has Teens`, `Age`, `Prediction_Probability`, and a `const` term.
*   A robust preprocessing function (`preprocess_inputs`) was developed to convert raw user inputs into the format expected by the Random Forest model. This function correctly handles the addition of a `const` column, scales numerical features such as `Income`, `Total_Expenditure`, and `Customer_Tenure_Months` using a pre-fitted `MinMaxScaler`, and applies one-hot encoding for categorical variables like `Education` and `Marital_Status` with `drop_first=True` logic, ensuring feature order consistency.
*   An interactive widget was successfully constructed using `ipywidgets`. This widget incorporates `FloatSlider` and `IntSlider` for numerical inputs, `Dropdown` for categorical selections (Education, Marital Status), and `Checkbox` for binary features (Complain, Has Kids, Has Teens). It also includes a "Predict Response" button that triggers the prediction process and displays the predicted campaign response probability and label.

### Insights or Next Steps
*   The interactive widget enables stakeholders to directly test the model with hypothetical customer profiles, fostering a deeper understanding of the factors influencing campaign response predictions.
*   For improved robustness, the next step should involve implementing input validation within the widget to ensure that user inputs adhere to realistic ranges and data types, thus preventing potential errors and improving user experience.

"""



"""# Task
Identified customer behavior-related features that will be used as inputs for the new widget. These features are: `Recency`, `NumDealsPurchases`, `NumWebPurchases`, `NumCatalogPurchases`, `Complain`, `Total_Expenditure`, `Average_Monthly_Spend`, and `Engagement_Score`.

## identify_behavior_features

### Subtask:
Identify and list only the customer behavior-related features from the full set of features that were used in the Random Forest model. These will be the inputs for the new widget.

**Reasoning**:
The subtask requires identifying specific customer behavior-related features from the model's feature list. I will filter the `rf_classifier.feature_names_in_` attribute to extract only the specified behavior-related features and then print them.
"""

model_features = rf_classifier.feature_names_in_

# Define the list of customer behavior-related features to identify
behavior_features_for_widget = [
    'Recency',
    'NumDealsPurchases',
    'NumWebPurchases',
    'NumCatalogPurchases',
    'Complain',
    'Total_Expenditure',
    'Average_Monthly_Spend',
    'Engagement_Score'
]

# Filter the model features to get only the behavior-related ones
identified_behavior_features = [f for f in model_features if f in behavior_features_for_widget]

print("Customer behavior-related features identified for the widget input:")
print(identified_behavior_features)

"""## create_behavior_preprocessing_function

### Subtask:
Develop a new preprocessing function that specifically handles the identified behavior-only inputs. This function will perform any necessary scaling and one-hot encoding for these selected features to prepare them for the Random Forest model.

**Reasoning**:
First, I need to prepare the data by creating `X_behavior` from `df3` using the identified behavior-related features, add a constant column, split the data into training and testing sets, and scale the numerical features. Then, I will train a new Random Forest Classifier on this processed behavior data. This will set up the necessary components before defining the preprocessing function itself.
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestClassifier
import statsmodels.api as sm

# 1. Create a new DataFrame, X_behavior, by selecting only the identified_behavior_features from df3.
#    The identified_behavior_features are already in the 'behavior_features_for_widget' list.
#    Ensure df3 does not contain 'const' from previous steps if it was temporarily added directly to df3 for earlier models.
#    The 'const' will be added specifically to X_behavior here.
X_behavior = df3[identified_behavior_features].copy()

# 2. Add a `const` column with a value of 1.0 to X_behavior
X_behavior = sm.add_constant(X_behavior, prepend=True)

# Define Y (Response column from df3)
Y = df3['Response']

# 3. Split the X_behavior DataFrame and the Y Series into training and testing sets
X_behavior_train, X_behavior_test, Y_train, Y_test = train_test_split(
    X_behavior, Y, test_size=0.3, random_state=42
)

# 4. Identify numerical columns within X_behavior_train (excluding 'const' and 'Complain')
numerical_behavior_cols = [col for col in X_behavior_train.columns
                           if col not in ['const', 'Complain'] and X_behavior_train[col].dtype != 'object']

scaler_behavior_numeric = MinMaxScaler()

# Fit and transform numerical columns in X_behavior_train
X_behavior_train[numerical_behavior_cols] = scaler_behavior_numeric.fit_transform(X_behavior_train[numerical_behavior_cols])

# Transform numerical columns in X_behavior_test
X_behavior_test[numerical_behavior_cols] = scaler_behavior_numeric.transform(X_behavior_test[numerical_behavior_cols])

# Ensure 'Complain' is integer type if it's not already (it should be 0 or 1)
X_behavior_train['Complain'] = X_behavior_train['Complain'].astype(int)
X_behavior_test['Complain'] = X_behavior_test['Complain'].astype(int)

# 5. Initialize and train a new RandomForestClassifier named rf_classifier_behavior
rf_classifier_behavior = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier_behavior.fit(X_behavior_train, Y_train)

print("X_behavior DataFrame created, split into train/test, numerical features scaled, and rf_classifier_behavior trained.")
print(f"X_behavior_train shape: {X_behavior_train.shape}")
print(f"X_behavior_test shape: {X_behavior_test.shape}")
print("First 5 rows of X_behavior_train (scaled numerical features):")
display(X_behavior_train.head())

"""**Reasoning**:
Now that the `rf_classifier_behavior` has been trained and the `scaler_behavior_numeric` is available, I will define the `preprocess_behavior_inputs` function. This function will take raw user inputs for the behavior features, add the constant, scale the numerical features using the fitted `scaler_behavior_numeric`, handle the `Complain` feature, and ensure the output is a DataFrame with features in the order expected by `rf_classifier_behavior`.


"""

def preprocess_behavior_inputs(
    Recency, NumDealsPurchases, NumWebPurchases, NumCatalogPurchases,
    Complain, Total_Expenditure, Average_Monthly_Spend, Engagement_Score
):
    # Create a dictionary from the raw inputs
    input_data = {
        'Recency': Recency,
        'NumDealsPurchases': NumDealsPurchases,
        'NumWebPurchases': NumWebPurchases,
        'NumCatalogPurchases': NumCatalogPurchases,
        'Complain': Complain,
        'Total_Expenditure': Total_Expenditure,
        'Average_Monthly_Spend': Average_Monthly_Spend,
        'Engagement_Score': Engagement_Score
    }

    # Convert to a DataFrame, ensuring it's a single row
    input_df = pd.DataFrame([input_data])

    # Add 'const' column at the beginning as expected by statsmodels and the model
    input_df.insert(0, 'const', 1.0)

    # Identify numerical columns for scaling (excluding 'const' and 'Complain')
    numerical_cols_to_scale = [col for col in input_df.columns
                               if col not in ['const', 'Complain'] and input_df[col].dtype != 'object']

    # Apply the fitted scaler to the numerical columns
    input_df[numerical_cols_to_scale] = scaler_behavior_numeric.transform(input_df[numerical_cols_to_scale])

    # Ensure 'Complain' is integer type
    input_df['Complain'] = input_df['Complain'].astype(int)

    # Ensure the order of columns matches what the model was trained on
    model_feature_order = list(rf_classifier_behavior.feature_names_in_)
    preprocessed_df = input_df[model_feature_order]

    return preprocessed_df

print("The 'preprocess_behavior_inputs' function has been defined.")

"""## build_behavior_widget

### Subtask:
Construct an interactive widget using `ipywidgets` that includes only the input fields corresponding to the identified customer behavior signals. This widget will use the new preprocessing function and the trained Random Forest model to display prediction probabilities.

## Summary:

### Data Analysis Key Findings

*   Eight customer behavior-related features were identified for the interactive widget input: `Recency`, `NumDealsPurchases`, `NumWebPurchases`, `NumCatalogPurchases`, `Complain`, `Total_Expenditure`, `Average_Monthly_Spend`, and `Engagement_Score`.
*   A new Random Forest model (`rf_classifier_behavior`) was trained specifically on these 8 behavior features, utilizing a dataset split into a training set of 1498 samples and a test set of 642 samples.
*   A `MinMaxScaler` was fitted to the numerical behavior features (excluding `const` and `Complain`) from the training data and applied consistently for scaling.
*   A dedicated preprocessing function, `preprocess_behavior_inputs`, was successfully developed. This function standardizes raw inputs by adding a constant, applying the pre-fitted `MinMaxScaler` to numerical features, and ensuring correct data types and feature order for prediction with the `rf_classifier_behavior` model.

### Insights or Next Steps

*   The newly trained Random Forest model (`rf_classifier_behavior`) and its corresponding `preprocess_behavior_inputs` function are now prepared to serve as the prediction engine for the interactive widget, ensuring consistent data handling for user inputs.
*   The next step is to construct the interactive `ipywidgets` interface, integrating the `preprocess_behavior_inputs` function to process user inputs and the `rf_classifier_behavior` model to generate and display prediction probabilities.

# Task
Create an interactive widget that allows users to input values for customer behavior-related features (`Recency`, `NumDealsPurchases`, `NumWebPurchases`, `NumCatalogPurchases`, `Complain`, `Total_Expenditure`, `Average_Monthly_Spend`, and `Engagement_Score`) and then displays the predicted campaign response probability using the `rf_classifier_behavior` model.

## apply_pca_3_components

### Subtask:
Apply PCA with 3 components to the `cluster_scaled` data and create a DataFrame for the results, including cluster labels.

**Reasoning**:
To apply PCA with 3 components as instructed, I need to import PCA from sklearn.decomposition, initialize it with n_components=3, then fit and transform the 'cluster_scaled' data, and finally create a DataFrame with the results and add the cluster labels.
"""

from sklearn.decomposition import PCA
import pandas as pd

# Initialize PCA with 3 components
pca_3d = PCA(n_components=3)

# Fit and transform the cluster_scaled data
cluster_pca_3d = pca_3d.fit_transform(cluster_scaled)

# Create a DataFrame for the 3D PCA results
pca_3d_df = pd.DataFrame(
    data=cluster_pca_3d,
    columns=['Principal_Component_1', 'Principal_Component_2', 'Principal_Component_3']
)

# Add the cluster labels to the PCA DataFrame
pca_3d_df['Cluster'] = kmeans.labels_

print(f"Shape of data after 3-component PCA: {pca_3d_df.shape}")
print("First 5 rows of 3-component PCA-transformed data with cluster labels:")
display(pca_3d_df.head())

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns

# Create a 3D plot
fig = plt.figure(figsize=(12, 10))
ax = fig.add_subplot(111, projection='3d')

# Get unique cluster labels and a color map
clusters = pca_3d_df['Cluster'].unique()
clusters.sort()
colors = sns.color_palette('viridis', n_colors=len(clusters))
color_map = dict(zip(clusters, colors))

# Plot each cluster separately
for cluster_id in clusters:
    subset = pca_3d_df[pca_3d_df['Cluster'] == cluster_id]
    ax.scatter(
        subset['Principal_Component_1'],
        subset['Principal_Component_2'],
        subset['Principal_Component_3'],
        label=f'Cluster {cluster_id}',
        color=color_map[cluster_id],
        s=100,
        alpha=0.7
    )

ax.set_title('Customer Clusters (3D PCA Visualization)')
ax.set_xlabel('Principal Component 1')
ax.set_ylabel('Principal Component 2')
ax.set_zlabel('Principal Component 3')
ax.legend(title='Cluster')
plt.tight_layout()
plt.show()

"""## visualize_pca_3_components

### Subtask:
Create a scatter plot of the first two principal components, colored by the `Cluster` labels, to visualize the cluster separation.

## Summary:

### Data Analysis Key Findings
*   Principal Component Analysis (PCA) was successfully applied to the `cluster_scaled` data, reducing its dimensionality to 3 components.
*   A new DataFrame, `pca_3d_df`, was created to store these 3 principal components along with the assigned cluster labels.
*   The `pca_3d_df` contains 2140 samples and 4 columns, comprising 'Principal\_Component\_1', 'Principal\_Component\_2', 'Principal\_Component\_3', and 'Cluster'.

### Insights or Next Steps
*   The `pca_3d_df` DataFrame is now prepared and can be used for visualization purposes, specifically to plot the first two principal components colored by their respective clusters to observe separation.
"""

